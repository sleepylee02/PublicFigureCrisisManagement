{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워드클라우드를 위한 text_dictionary 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os \n",
    "\n",
    "# 임의 데이터 형식 통일 \n",
    "# 하나의 파일로 데이터 다 통합하기\n",
    "\n",
    "\"\"\"뉴스 영상 스크래핑 구간 \n",
    "위기발생 - 1달 ~ 위기 대응 + 1달\n",
    "\n",
    "유튜브 영상 스크래핑 구간\n",
    "1. 위기발생 전 2년 ~ 위기 발생 전날\n",
    "2. 위기 발생 당일 ~ 위기 대응 전날\n",
    "3. 위기 대응 날 ~ 1달\"\"\"\n",
    "\n",
    "information = [[\"아이린\",\"2020.10.20\",\"2020.10.22\"],[\"조현아\",\"2014.12.05\",\"2014.12.08\"],[\"박나래\",\"2021.03.24\",\"2021.03.25\"],[\"설현\",\"2016.05.03\",\"2016.05.12\"],\n",
    "               [\"양팡\",\"2020.08.05\",\"2020.08.08\"],[\"강민경\",\"2020.07.15\",\"2020.07.17\"],[\"유희열\",\"2022.06.14\",\"2022.06.14\"],\n",
    "               [\"홍진영\",\"2020.11.05\",\"2020.11.06\"],[\"설민석\",\"2020.12.29\",\"2020.12.29\"]]\n",
    "\n",
    "for i in range(len(information)):\n",
    "    information[i][1] = pd.to_datetime(information[i][1]).date()\n",
    "    information[i][2] = pd.to_datetime(information[i][2]).date()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 df으로 read 시켜주기\n",
    "\n",
    "naver = pd.read_csv(\"naver.csv\")\n",
    "naver_divided_comment = pd.read_csv(\"naver_divided_comment.csv\")\n",
    "yt_transcript = pd.read_csv(\"yt_transcript.csv\")\n",
    "yt_comment = pd.read_csv(\"yt_comment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_id로 댓글에 published date 달아주기\n",
    "yt_comment = yt_comment.merge(yt_transcript.loc[:,[\"Video_ID\",\"Publish Date\"]], on=\"Video_ID\",how=\"inner\")\n",
    "\n",
    "# df내 object들을 datetime object로 바꿔주기\n",
    "naver[\"Date\"] = pd.to_datetime(naver[\"Date\"]).dt.date\n",
    "yt_transcript[\"Publish Date\"] = pd.to_datetime(yt_transcript[\"Publish Date\"]).dt.date\n",
    "yt_comment[\"Publish Date\"] = pd.to_datetime(yt_comment[\"Publish Date\"]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 기간별로 text 나눠서 text 분석을 할 수 있도록 dictionary꼴로 만들어줌!\n",
    "text_dictionary = {}\n",
    "\n",
    "for info in information:\n",
    "    query = info[0]\n",
    "\n",
    "    naver_comment1 = naver.loc[(naver[\"query\"]==query) & (naver[\"Date\"] < info[1]) , \"comments\"]\n",
    "    naver_comment1 = \" \".join(naver_comment1.dropna().astype(str))\n",
    "\n",
    "    naver_comment2 = naver.loc[(naver[\"query\"]==query) & (naver[\"Date\"] >= info[1]) & (naver[\"Date\"] < info[2]), \"comments\"]\n",
    "    naver_comment2 = \" \".join(naver_comment2.dropna().astype(str))\n",
    "\n",
    "    naver_comment3 = naver.loc[(naver[\"query\"]==query) & (naver[\"Date\"] >= info[2]), \"comments\"]\n",
    "    naver_comment3 = \" \".join(naver_comment3.dropna().astype(str))\n",
    "    \n",
    "\n",
    "    naver_content1 = naver.loc[(naver[\"query\"]==query) & (naver[\"Date\"] < info[1]) , \"Content\"]\n",
    "    naver_content1 = \" \".join(naver_content1.dropna().astype(str))\n",
    "\n",
    "    naver_content2 = naver.loc[(naver[\"query\"]==query) & (naver[\"Date\"] >= info[1]) & (naver[\"Date\"] < info[2]), \"Content\"]\n",
    "    naver_content2 = \" \".join(naver_content2.dropna().astype(str))\n",
    "\n",
    "    naver_content3 = naver.loc[(naver[\"query\"]==query) & (naver[\"Date\"] >= info[2]), \"Content\"]\n",
    "    naver_content3 = \" \".join(naver_content3.dropna().astype(str))\n",
    "\n",
    "    \n",
    "    youtube_transcript1 = yt_transcript.loc[(yt_transcript[\"query\"]==query) & (yt_transcript[\"Publish Date\"] < info[1]) , \"Transcript\"]\n",
    "    youtube_transcript1 = \" \".join(youtube_transcript1.dropna().astype(str))\n",
    "\n",
    "    youtube_transcript2 = yt_transcript.loc[(yt_transcript[\"query\"]==query) & (yt_transcript[\"Publish Date\"] >= info[1]) & (yt_transcript[\"Publish Date\"] < info[2]) , \"Transcript\"]\n",
    "    youtube_transcript2 = \" \".join(youtube_transcript2.dropna().astype(str))\n",
    "    \n",
    "    youtube_transcript3 = yt_transcript.loc[(yt_transcript[\"query\"]==query) & (yt_transcript[\"Publish Date\"] >= info[2]) , \"Transcript\"]\n",
    "    youtube_transcript3 = \" \".join(youtube_transcript3.dropna().astype(str))\n",
    "\n",
    "    youtube_comment1 = yt_comment.loc[(yt_comment[\"query\"]==query) & (yt_comment[\"Publish Date\"] < info[1]) , \"comment_text\"]\n",
    "    youtube_comment1 = \" \".join(youtube_comment1.dropna().astype(str))\n",
    "\n",
    "    youtube_comment2 = yt_comment.loc[(yt_comment[\"query\"]==query) & (yt_comment[\"Publish Date\"] >= info[1]) & (yt_comment[\"Publish Date\"] < info[2]) , \"comment_text\"]\n",
    "    youtube_comment2 = \" \".join(youtube_comment2.dropna().astype(str))\n",
    "    \n",
    "    youtube_comment3 = yt_comment.loc[(yt_comment[\"query\"]==query) & (yt_comment[\"Publish Date\"] >= info[2]) , \"comment_text\"]\n",
    "    youtube_comment3 = \" \".join(youtube_comment3.dropna().astype(str))\n",
    "\n",
    "    new_dict = {\n",
    "                \"naver_comment1\" : naver_comment1,\"naver_comment2\" : naver_comment2,\"naver_comment3\" : naver_comment3,\n",
    "                \"naver_content1\" : naver_content1,\"naver_content2\" : naver_content2,\"naver_content3\" : naver_content3,\n",
    "                \"youtube_transcipt1\" : youtube_transcript1,\"youtube_transcipt2\" : youtube_transcript2,\"youtube_transcipt3\" : youtube_transcript3, \n",
    "                \"youtube_comment1\" : youtube_comment1,\"youtube_comment2\" : youtube_comment2,\"youtube_comment3\" : youtube_comment3\n",
    "                }\n",
    "    text_dictionary[query] = new_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 워드클라우드 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naver1, 2, 3 ....형태가 아니라 인물별로 시기1 시기2 시기3로 만들기\n",
    "\n",
    "def update_text_dictionary(text_dictionary):\n",
    "    for query, content_dict in text_dictionary.items():\n",
    "        text_dictionary[query] = {\n",
    "            \"시기1\": ' '.join([content_dict.get(f'{source}1', '') for source in ['naver_comment', 'naver_content', 'youtube_transcript', 'youtube_comment']]),\n",
    "            \"시기2\": ' '.join([content_dict.get(f'{source}2', '') for source in ['naver_comment', 'naver_content', 'youtube_transcript', 'youtube_comment']]),\n",
    "            \"시기3\": ' '.join([content_dict.get(f'{source}3', '') for source in ['naver_comment', 'naver_content', 'youtube_transcript', 'youtube_comment']]),\n",
    "        }\n",
    "\n",
    "update_text_dictionary(text_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/m5/bfzh8qks2fx0rrw3lkq1q4s80000gn/T/ipykernel_58418/1368753592.py:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  filtered_content = re.sub('[^\\s\\w\\d]', ' ', text)\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "import pandas as pd\n",
    "from kiwipiepy.utils import Stopwords\n",
    "import re\n",
    "\n",
    "kiwi = Kiwi()\n",
    "stopwords = Stopwords()\n",
    "\n",
    "# 전처리 코드\n",
    "def do_Kr_preprocessing(text, include_tags=['NN', 'NNG', 'NNP', 'MAG',\"VA\"]):\n",
    "    filtered_content = re.sub('[^\\s\\w\\d]', ' ', text)\n",
    "    kiwi_tokens = kiwi.tokenize(filtered_content, stopwords=stopwords)\n",
    "    selected_words = [token.form for token in kiwi_tokens if token.tag in include_tags and len(token.form) > 1 and len(token.form) < 180]\n",
    "    return selected_words\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# 빈도 분석!\n",
    "\n",
    "columns = ['query', 'key', 'count']\n",
    "freq_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for info in information:\n",
    "    query = info[0]\n",
    "    for key in list(text_dictionary[query].keys()):\n",
    "        conversion = text_dictionary[query][key]\n",
    "        final_nouns = do_Kr_preprocessing(conversion)\n",
    "        c = Counter(final_nouns)\n",
    "        adding_df = pd.DataFrame({\"query\" : [query], \"key\" : [key], \"count\" : [str(c.most_common(30))]})\n",
    "        freq_df = pd.concat([freq_df,adding_df], ignore_index=True)\n",
    "\n",
    "freq_df.to_csv(\"frequency_analysis2.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import ast\n",
    "\n",
    "plt.rcParams['font.family'] = 'AppleGothic'  # 한글 폰트 설정\n",
    "file_path = 'frequency_analysis2.csv'\n",
    "sent_df = pd.read_csv(file_path)\n",
    "\n",
    "# 문자열 리스트를 딕셔너리로 변환\n",
    "sent_df['count'] = sent_df['count'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else [])\n",
    "\n",
    "def soft_color_func(*args, **kwargs):\n",
    "    return \"hsl(%d, 75%%, 50%%)\" % np.random.choice(range(0, 200))  # HSL 색상 중 파스텔 블루-그린\n",
    "\n",
    "def plot_wordcloud(pdf):\n",
    "    for idx, row in sent_df.iterrows():\n",
    "        if row['count']:  # 값이 있는 경우만 시각화\n",
    "            word_freq = dict(row['count'])\n",
    "            wordcloud = WordCloud(\n",
    "                font_path='/Library/Fonts/AppleGothic.ttf',\n",
    "                background_color='white',\n",
    "                width=500,  # 정사각형 비율\n",
    "                height=500,\n",
    "                max_words=200,\n",
    "                color_func=soft_color_func,  # 색상 함수 적용\n",
    "                contour_color='gray',\n",
    "                contour_width=1\n",
    "            ).generate_from_frequencies(word_freq)\n",
    "            \n",
    "            plt.figure(figsize=(6, 6))  # 정사각형 플롯\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.title(f\"Query: {row['query']} | Key: {row['key']}\", fontsize=14, color='black')\n",
    "            plt.axis('off')  # 축 제거\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "\n",
    "def save_plots_to_pdf(file_path, plot_function):\n",
    "    with PdfPages(file_path) as pdf:\n",
    "        plot_function(pdf)\n",
    "\n",
    "save_plots_to_pdf('frequency_analysis_wordcloud_report4.pdf', plot_wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추세선 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment들 일일이 라벨링하고 라벨링별 (1, 0, -1) 평균내기\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "file_path = 'sent_with_date_final.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define sentiment categories\n",
    "positive = set(['감동/감탄', '기대감', '신기함/관심', '존경', '안심/신뢰', '환영/호의', '뿌듯함', \n",
    "                '고마움', '행복', '아껴주는', '기쁨', '흐뭇함(귀여움/예쁨)', '즐거움/신남', '편안/쾌적'])\n",
    "negative = set(['안타까움/실망', '의심/불신', '절망', '불평/불만', '화남/분노', '경악', '공포/무서움', \n",
    "                '당황/난처', '부끄러움', '부담/안내킴', '서러움', '역겨움/징그러움', '우쭐댐/무시함', \n",
    "                '증오/혐오', '지긋지긋', '짜증', '패배/자기혐오', '한심함', '힘듦/지침', '슬픔', '불안/걱정', '귀찮음'])\n",
    "neutral = set(['깨달음', '없음', '비장함', '놀람', '신기함/관심', '죄책감', '불쌍함/연민', '어이없음', '재미없음'])\n",
    "\n",
    "\n",
    "\n",
    "def calculate_weighted_averages(sentiment_list):\n",
    "    sentiments = re.findall(r\"\\['(.*?)', ([0-9.]+)\\]\", sentiment_list)\n",
    "    if not sentiments:\n",
    "        return 0, 0, 0  # Handle empty sentiment lists\n",
    "\n",
    "    positive_scores = [float(score) for sentiment, score in sentiments if sentiment in positive]\n",
    "    negative_scores = [-float(score) for sentiment, score in sentiments if sentiment in negative]\n",
    "    neutral_scores = [float(score) for sentiment, score in sentiments if sentiment in neutral]\n",
    "\n",
    "    total_scores = len(sentiments)\n",
    "\n",
    "    pos_avg = sum(positive_scores) / total_scores if total_scores else 0\n",
    "    neg_avg = sum(negative_scores) / total_scores if total_scores else 0\n",
    "    neu_avg = sum(neutral_scores) / total_scores if total_scores else 0\n",
    "\n",
    "    return pos_avg, neu_avg, neg_avg\n",
    "\n",
    "# Apply the function and create new columns\n",
    "data[['positive_avg', 'neutral_avg', 'negative_avg']] = data['sentiment'].dropna().apply(\n",
    "    lambda x: pd.Series(calculate_weighted_averages(x))\n",
    ")\n",
    "\n",
    "# Save the processed data\n",
    "weighted_output_path = 'sent_with_weighted_sentiment_averages2.csv'\n",
    "data.to_csv(weighted_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추세선 그리기\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "import numpy as np\n",
    "\n",
    "# Load data from a local CSV file\n",
    "data = pd.read_csv('sent_with_weighted_sentiment_averages2.csv')\n",
    "\n",
    "# Convert 'Date' to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "data = data.dropna(subset=['Date'])\n",
    "\n",
    "# Ensure only numeric columns are aggregated\n",
    "numeric_columns = ['positive_avg', 'neutral_avg', 'negative_avg']\n",
    "grouped_data = data.groupby(['query', 'type', 'Date'])[numeric_columns].mean().reset_index()\n",
    "\n",
    "# Correct negative_avg to be negative\n",
    "grouped_data['negative_avg'] *= -1\n",
    "\n",
    "# Filter only specified queries and types\n",
    "queries_of_interest = ['강민경', '박나래', '설현', '아이린', '설민석', '홍진영', '양팡', '유희열', '조현아']\n",
    "filtered_data = grouped_data[grouped_data['query'].isin(queries_of_interest)]\n",
    "\n",
    "# Unique combinations of query and type\n",
    "query_type_combinations = filtered_data[['query', 'type']].drop_duplicates()\n",
    "\n",
    "# Define significant events by query\n",
    "significant_events = {\n",
    "    '아이린': {'2020-10-20': '논란 발생', '2020-10-22': '사과문'},\n",
    "    '조현아': {'2014-12-05': '논란 발생', '2014-12-08': '사과문'},\n",
    "    '박나래': {'2021-03-24': '논란 발생', '2021-03-25': '사과문', '2021-04-09': '활동 재개'},\n",
    "    '설현': {'2016-05-03': '논란 발생', '2016-05-12': '사과문'},\n",
    "    '설민석': {'2020-12-29': '논란 발생 및 사과문 기재'},\n",
    "    '양팡': {'2020-08-05': '논란 발생', '2020-08-08': '사과문'},\n",
    "    '유희열': {'2022-06-14': '논란 발생 및 1차 사과문 게재', '2022-07-18': '2차 사과문 게재'},\n",
    "    '홍진영': {'2020-11-05': '논란 발생', '2020-11-06': '부인', '2020-12-18': '사과문'},\n",
    "    '강민경': {'2020-07-15': '논란 발생', '2020-07-17': '사과문'}\n",
    "}\n",
    "\n",
    "# Set font for MacOS\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "\n",
    "# Create a PDF to save all plots\n",
    "pdf_path_corrected = 'sentiment_trends_all_types.pdf'\n",
    "\n",
    "# Re-plot graphs and save to PDF\n",
    "def plot_each_row(pdf, max_points=30, cutoff_days=60):\n",
    "    for _, row in query_type_combinations.iterrows():\n",
    "        query, query_type = row['query'], row['type']\n",
    "        subset = filtered_data[(filtered_data['query'] == query) & (filtered_data['type'] == query_type)]\n",
    "\n",
    "        if not subset.empty:\n",
    "            if query in significant_events:\n",
    "                first_event_date = pd.to_datetime(list(significant_events[query].keys())[0])\n",
    "                date_range_start = first_event_date - pd.DateOffset(days=cutoff_days)\n",
    "                date_range_end = first_event_date + pd.DateOffset(days=cutoff_days)\n",
    "                subset = subset[(subset['Date'] >= date_range_start) & (subset['Date'] <= date_range_end)]\n",
    "\n",
    "            if len(subset) > max_points:\n",
    "                subset = subset.sample(n=max_points, random_state=42).sort_values(by='Date')\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # Original data points\n",
    "            plt.scatter(subset['Date'], subset['positive_avg'], label='긍정 평균', color='b', alpha=0.3)\n",
    "            plt.scatter(subset['Date'], subset['neutral_avg'], label='중립 평균', color='g', alpha=0.3)\n",
    "            plt.scatter(subset['Date'], subset['negative_avg'], label='부정 평균', color='r', alpha=0.3)\n",
    "\n",
    "            # Rolling Mean (Smoothing)\n",
    "            window_size = max(5, len(subset) // 10)\n",
    "            for label, y_values, color in [\n",
    "                ('긍정 추세', subset['positive_avg'], 'b'),\n",
    "                ('중립 추세', subset['neutral_avg'], 'g'),\n",
    "                ('부정 추세', subset['negative_avg'], 'r'),\n",
    "            ]:\n",
    "                rolling_avg = y_values.rolling(window=window_size, min_periods=1).mean()\n",
    "                plt.plot(subset['Date'], rolling_avg, color=color, label=label)\n",
    "\n",
    "            # Add significant event lines with labels\n",
    "            if query in significant_events:\n",
    "                colors = ['orange', 'purple', 'brown', 'red']\n",
    "                for idx, (event_date, event_label) in enumerate(significant_events[query].items()):\n",
    "                    event_date_parsed = pd.to_datetime(event_date)\n",
    "                    plt.axvline(event_date_parsed, color=colors[idx % len(colors)], linestyle='--', linewidth=2, label=event_label)\n",
    "\n",
    "            plt.title(f'{query} ({query_type}) 감정 추세 (Rolling Mean)')\n",
    "            plt.xlabel('날짜')\n",
    "            plt.ylabel('평균 감정 점수')\n",
    "            plt.axhline(0, color='black', linestyle='--', alpha=0.7)\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "            plt.gcf().autofmt_xdate()\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "\n",
    "# Save plots to the PDF file\n",
    "def save_plots_to_pdf(file_path, plot_function):\n",
    "    with matplotlib.backends.backend_pdf.PdfPages(file_path) as pdf:\n",
    "        plot_function(pdf)\n",
    "\n",
    "# Generate the corrected PDF\n",
    "save_plots_to_pdf(pdf_path_corrected, plot_each_row)\n",
    "\n",
    "print(f\"PDF 파일이 생성되었습니다: {pdf_path_corrected}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시기별 댓글 수 파악하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "\n",
    "# Load data from a local CSV file\n",
    "file_path = 'sent_with_weighted_sentiment_averages2.csv'  # 파일 경로 수정\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'Date' to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "data = data.dropna(subset=['Date'])\n",
    "\n",
    "# Define queries of interest and significant events\n",
    "queries_of_interest = ['강민경', '박나래', '설현', '아이린', '설민석', '홍진영', '양팡', '유희열', '조현아']\n",
    "significant_events = {\n",
    "    '아이린': {'2020-10-20': '논란 발생', '2020-10-22': '사과문'},\n",
    "    '조현아': {'2014-12-05': '논란 발생', '2014-12-08': '사과문'},\n",
    "    '박나래': {'2021-03-24': '논란 발생', '2021-03-25': '사과문', '2021-04-09': '활동 재개'},\n",
    "    '설현': {'2016-05-03': '논란 발생', '2016-05-12': '사과문'},\n",
    "    '설민석': {'2020-12-29': '논란 발생 및 사과문 기재'},\n",
    "    '양팡': {'2020-08-05': '논란 발생', '2020-08-08': '사과문'},\n",
    "    '유희열': {'2022-06-14': '논란 발생 및 1차 사과문 게재', '2022-07-18': '2차 사과문 게재'},\n",
    "    '홍진영': {'2020-11-05': '논란 발생', '2020-11-06': '부인', '2020-12-18': '사과문'},\n",
    "    '강민경': {'2020-07-15': '논란 발생', '2020-07-17': '사과문'}\n",
    "}\n",
    "\n",
    "# Filter data to only relevant queries and types\n",
    "filtered_data = data[data['query'].isin(queries_of_interest)]\n",
    "\n",
    "# Count comments by date for each query\n",
    "comment_counts = filtered_data.groupby(['query', 'Date']).size().reset_index(name='count')\n",
    "\n",
    "# Set font for MacOS\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "\n",
    "# Create a PDF to save all plots\n",
    "pdf_path_counts = 'comment_counts_by_date_dark_green.pdf'\n",
    "\n",
    "# Plot counts by date and save to PDF\n",
    "def plot_comment_counts(pdf):\n",
    "    for query in queries_of_interest:\n",
    "        query_data = comment_counts[comment_counts['query'] == query]\n",
    "\n",
    "        # Calculate x-axis range based on significant events\n",
    "        if query in significant_events:\n",
    "            first_event_date = pd.to_datetime(list(significant_events[query].keys())[0])\n",
    "            last_event_date = pd.to_datetime(list(significant_events[query].keys())[-1])\n",
    "            date_range_start = first_event_date - pd.DateOffset(months=1)\n",
    "            date_range_end = last_event_date + pd.DateOffset(months=1)\n",
    "            query_data = query_data[(query_data['Date'] >= date_range_start) & (query_data['Date'] <= date_range_end)]\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(query_data['Date'], query_data['count'], marker='o', color='#228B22', label='댓글 수')\n",
    "\n",
    "        # Add significant event lines with labels\n",
    "        if query in significant_events:\n",
    "            colors = ['orange', 'purple', 'brown', 'red']\n",
    "            for idx, (event_date, event_label) in enumerate(significant_events[query].items()):\n",
    "                event_date_parsed = pd.to_datetime(event_date)\n",
    "                plt.axvline(event_date_parsed, color=colors[idx % len(colors)], linestyle='--', linewidth=2, label=event_label)\n",
    "\n",
    "        plt.title(f'{query} 댓글 수 추이')\n",
    "        plt.xlabel('날짜')\n",
    "        plt.ylabel('댓글 수')\n",
    "        plt.axhline(0, color='black', linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "\n",
    "# Save plots to the PDF file\n",
    "def save_plots_to_pdf(file_path, plot_function):\n",
    "    with matplotlib.backends.backend_pdf.PdfPages(file_path) as pdf:\n",
    "        plot_function(pdf)\n",
    "\n",
    "# Generate the corrected PDF\n",
    "save_plots_to_pdf(pdf_path_counts, plot_comment_counts)\n",
    "\n",
    "print(f\"PDF 파일이 생성되었습니다: {pdf_path_counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 빈도분석 단순 count 표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m5/bfzh8qks2fx0rrw3lkq1q4s80000gn/T/ipykernel_58418/2488724768.py:84: UserWarning: Glyph 8722 (\\N{MINUS SIGN}) missing from font(s) AppleGothic.\n",
      "  pdf.savefig(fig)\n",
      "/var/folders/m5/bfzh8qks2fx0rrw3lkq1q4s80000gn/T/ipykernel_58418/2488724768.py:84: UserWarning: Glyph 8722 (\\N{MINUS SIGN}) missing from font(s) AppleGothic.\n",
      "  pdf.savefig(fig)\n",
      "/var/folders/m5/bfzh8qks2fx0rrw3lkq1q4s80000gn/T/ipykernel_58418/2488724768.py:84: UserWarning: Glyph 8722 (\\N{MINUS SIGN}) missing from font(s) AppleGothic.\n",
      "  pdf.savefig(fig)\n",
      "/var/folders/m5/bfzh8qks2fx0rrw3lkq1q4s80000gn/T/ipykernel_58418/2488724768.py:84: UserWarning: Glyph 8722 (\\N{MINUS SIGN}) missing from font(s) AppleGothic.\n",
      "  pdf.savefig(fig)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 파일이 생성되었습니다: query_word_frequencies_narrow_columns.pdf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Load data from a local CSV file\n",
    "file_path = 'frequency_analysis2.csv'  # 파일 경로 수정\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Set font for MacOS\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "\n",
    "# Function to split 'count' column into word-frequency pairs\n",
    "def split_count_column(row):\n",
    "    try:\n",
    "        return ast.literal_eval(row)  # Safely evaluate as a list of tuples\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Apply the splitting function to the 'count' column\n",
    "data['count'] = data['count'].apply(split_count_column)\n",
    "\n",
    "# Create a DataFrame to store query, key (시기), word, and frequency\n",
    "query_word_list = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    query = row['query']\n",
    "    key = row['key']  # 시기1, 시기2, 시기3\n",
    "    word_freq_list = row['count']\n",
    "    for word, freq in word_freq_list:\n",
    "        query_word_list.append({'query': query, 'key': key, 'word': word, 'count': freq})\n",
    "\n",
    "# Convert to DataFrame\n",
    "word_counts = pd.DataFrame(query_word_list)\n",
    "\n",
    "# Function to merge 시기1, 시기2, 시기3 into three columns sorted by descending count\n",
    "def combine_period_columns(word_counts, query):\n",
    "    tables = []\n",
    "    for key in ['시기1', '시기2', '시기3']:\n",
    "        period_data = word_counts[(word_counts['query'] == query) & (word_counts['key'] == key)]\n",
    "        # Sort by descending count\n",
    "        period_data = period_data.sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "        tables.append(period_data.apply(lambda row: f\"{row['word']}({row['count']})\", axis=1).tolist())\n",
    "\n",
    "    # Ensure all tables have the same number of rows by padding with empty values\n",
    "    max_length = max(len(table) for table in tables)\n",
    "    for table in tables:\n",
    "        while len(table) < max_length:\n",
    "            table.append('')  # Add empty strings for missing values\n",
    "\n",
    "    # Combine tables into a single table with three columns\n",
    "    combined_table = []\n",
    "    for i in range(max_length):\n",
    "        combined_table.append([tables[0][i], tables[1][i], tables[2][i]])\n",
    "    return combined_table\n",
    "\n",
    "# Create PDF path\n",
    "pdf_file_path = 'query_word_frequencies_narrow_columns.pdf'\n",
    "\n",
    "# Generate PDF\n",
    "with PdfPages(pdf_file_path) as pdf:\n",
    "    for query in word_counts['query'].unique():\n",
    "        # Merge 시기1, 시기2, 시기3 into a single table\n",
    "        combined_table = combine_period_columns(word_counts, query)\n",
    "\n",
    "        # Define column headers\n",
    "        col_labels = ['시기1', '시기2', '시기3']\n",
    "\n",
    "        # Plot combined table\n",
    "        fig, ax = plt.subplots(figsize=(8, min(12, 0.5 * len(combined_table) + 1)))  # Adjust height dynamically\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        table = ax.table(cellText=combined_table, colLabels=col_labels, loc='center', cellLoc='center',\n",
    "                         colWidths=[0.2, 0.2, 0.2])  # Narrower column widths\n",
    "\n",
    "        # Set title\n",
    "        ax.set_title(f'{query} 시기별 단어 빈도수', fontsize=14, fontweight='bold')\n",
    "\n",
    "        # Adjust table formatting\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "\n",
    "        # Save page to PDF\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(f\"PDF 파일이 생성되었습니다: {pdf_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
